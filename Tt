import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from darts.datasets import AirPassengersDataset
from darts import TimeSeries
from transformers import LlamaForCausalLM, LlamaTokenizer

# ==========================
# Étape 1 : Charger les données
# ==========================
def prepare_data():
    # Charger une série temporelle univariée (AirPassengers)
    series = AirPassengersDataset().load()
    
    # Normalisation des données
    series = series.normalize()
    
    # Découpage en train/test
    train, test = series.split_before(0.8)
    return train, test

train, test = prepare_data()

# ==========================
# Étape 2 : Obtenir les représentations de Llama
# ==========================
def series_to_text(series):
    # Convertir une série temporelle en texte
    return ",".join(map(str, series.values().flatten().tolist()))

# Charger le modèle Llama
model_name = "decapoda-research/llama-7b-hf"  # Remplacez par le modèle Llama de votre choix
llama = LlamaForCausalLM.from_pretrained(model_name)
tokenizer = LlamaTokenizer.from_pretrained(model_name)

def get_llama_representations(series):
    # Convertir la série en texte
    text = series_to_text(series)
    
    # Transformer le texte en représentations via Llama
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = llama(**inputs, output_hidden_states=True)
    
    # Extraire les états cachés de la dernière couche
    hidden_states = outputs.hidden_states[-1]  # (batch_size, seq_length, hidden_dim)
    return hidden_states.squeeze(0).cpu().numpy()

# Obtenir les représentations pour l'ensemble d'entraînement
llama_representations = get_llama_representations(train)

# ==========================
# Étape 3 : Définir le modèle CNN
# ==========================
class CNNStudent(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(CNNStudent, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, padding=1)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = x.mean(dim=-1)  # Global average pooling
        x = self.fc(x)
        return x

# Initialiser le modèle
input_dim = 1  # Série univariée
hidden_dim = 64
output_dim = llama_representations.shape[1]  # Taille des représentations Llama
cnn = CNNStudent(input_dim, hidden_dim, output_dim)

# ==========================
# Étape 4 : Entraîner le modèle CNN
# ==========================
def train_cnn(cnn, train, llama_representations, epochs=10, lr=1e-3):
    optimizer = optim.Adam(cnn.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    # Préparer les données d'entrée
    train_inputs = torch.tensor(train.values(), dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, seq_length)
    targets = torch.tensor(llama_representations, dtype=torch.float32)  # (batch_size, seq_length, hidden_dim)

    # Boucle d'entraînement
    for epoch in range(epochs):
        cnn.train()
        optimizer.zero_grad()

        # Passer les données dans le modèle
        outputs = cnn(train_inputs)
        
        # Calculer la perte
        loss = loss_fn(outputs, targets)
        
        # Backpropagation
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}")

train_cnn(cnn, train, llama_representations)

# ==========================
# Étape 5 : Évaluer le modèle
# ==========================
def evaluate_cnn(cnn, test):
    cnn.eval()
    test_inputs = torch.tensor(test.values(), dtype=torch.float32).unsqueeze(1)  # (batch_size, 1, seq_length)
    with torch.no_grad():
        predictions = cnn(test_inputs)
    return predictions

# Obtenir les prédictions
predictions = evaluate_cnn(cnn, test)
print("Prédictions CNN Shape :", predictions.shape)

# ==========================
# Étape 6 : Sauvegarder le modèle
# ==========================
torch.save(cnn.state_dict(), "cnn_student_model.pth")
print("Modèle sauvegardé sous cnn_student_model.pth")
